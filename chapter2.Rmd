# Exercises for week 2

*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using  

First we name our dataset learning2014 and read our dataset that will be used for analysis. The dataset contains information about students who have taken an exam for a specific course (Johdatus yhteiskuntatilastotieteeseen). In the dataset we have combined information about each participating student's gender, age, attitude, deep learning score, strategic learning score, surface learning score and exam points. Scores for attitude, and deep, strategic and surface learning have been formulated from each student's responses to a questionnare. These scores are shown using the Likert scale with 5 levels.


```{r dataset}
learning2014 <- read.table("/Users/Noora/Documents/IODS-project/data/learning2014.txt", header = TRUE)
learning2014
```

We can explore the dimensions of the dataset to find out how many rows and columns it has.
```{r dimensions of dataset}
dim(learning2014)
```
This shows that the dataset has 166 rows (observations) and 7 columns (variables).

We can also explore the structure of the dataset.
```{r structure of dataset}
str(learning2014)
```
We can see again that the dataset has 166 observations and 7 variables, but we can now also see the names of the variables (e.g. gender, age) and the type of data they contain (e.g. integers for age) and some of the first observations.

Next, we look at a graphical overview of the data.
``` {r graphical overview}
library(GGally)
library(ggplot2)
GraphicalOverview <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.5), title = "Graphical Overview", lower = list(combo = wrap("facethist", bins = 20)))
GraphicalOverview
```
Analysis text....
 
We can also get summaries of the variables.

Summary of variable gender:
``` {r summary of gender}
summary(learning2014$gender)
```

Summary of variable age:
``` {r summary of age}
summary(learning2014$age)
```

Summary of variable attitude:
``` {r summary of attitude}
summary(learning2014$attitude)
```

Summary of variable deep:
``` {r summary of deep}
summary(learning2014$deep)
```

Summary of variable stra:
``` {r summary of stra}
summary(learning2014$stra)
```

Summary of variable surf:
``` {r summary of surf}
summary(learning2014$surf)
```

Summary of variable points:
``` {r summary of points}
summary(learning2014$points)
```

Analysis here....

Next we will create a regression model with three explanatory variables age, gender and attitude, and exam points as the target variable. In other words, we are trying to invstigate if the variables age, gender and attitude can be used to predict the value of exam points.

We will conduct some tests on our model to see if the three variables can be used to predict the value of exam points or not.
``` {r multiple regression summary}
Model <- lm(points ~ age + gender + attitude, data = learning2014)
summary(Model)
```

The results of this test indicate that the null hypothesis that the variable's coefficient is 0 (i.e. the variable doesn't predict exam points) is valid for age and gender. Based on this information we should remove these two variables.

JYT ei saa katsoa liikaa P arvoja.

Let's conduct the same test but this time with just the variable attitude.
``` {r multiple regression summary2}
Model2 <- lm(points ~ attitude, data = learning2014)
summary(Model2)
```
According to this test, attitude is statistically significant, so we keep it in out model!

Based on the output of the summary we see that the effect of attitude on exam points is 3.5. In other words, if attitude improves by 1, exam points improve by 3.5.

We can also see the value of parameter *a* (alpha), i.e. the point where the regression line intercepts the y-axis. This value in our model is 11.6. This implies that a person with attitude 0 would get 11.6 points from the exam. We can also see that the standard error of attitude is 0.6 and the P value very small (4.12 x 10^-09).

The multiple r-squared value tells how well the model explains the target variable exam points. We see that around 19% of the variation of points is explained by attitude. We can say that attitude seems to contribute to success in the exam (exam points), but there are also clearly other variables that affect success in the exam as well.


We can also draw diagnostic plots about our model. We want the diagnostic plots to sho residuals vs. fitted values, normal QQ-plot and residuals vs leverage. These will be explained in analysis of the diagnostic plots.
``` {r multiple regression diagnostics}
par(mfrow=c(2,2))
plot(Model2, which = c(1, 2, 5))
```

Because our model is about linear regression, it assumes that there is a linear relationship between attitude and exam points. Another important assumption is that there is a random variable (*e*) that adds noise to the observations This *e* describes the errors of the model. The linear regression model has several assumptions related to the errors. It is assumed that the errors 1) are normally distributed, 2) aren't correlatd, and 3) have constant variance (i.e. the size of an error doesn't depend on the explanatory variables, in this case attitude).

Analysing the residuals of the model allows us to investigate the validity of these assumptions.

The first plot, residuals vs. fitted, can be used to evaluate if the errors have constant variance. If there is any pattern in the scatter plot, there is a problem with the assumptions of the model. However, there doesn't appear to be any pattern in our scatter plot, the spread of points seems random. Thus the plot doesn't imply there would be problems with our model's assumptions.

The second plot, normal QQ, can be used to evaluate the normality of the errors. The better the points fit to the line in the plot, the better they fit the normality assumption. From the QQ-plot we can see that most of the findings fit the normality assumption, except for in the beginning and end of the line, where there is more deviation from the line. Thus the normality assumption might be questionable.

The third plot, residuals vs. leverage, can be used to see how great impact a single obsrvation has on the model. There might be e.g. one observation that is causing the slope of the regression line to significantly change. In our plot, there are a couple of observations at the far right side of the plot. However, the leverage of these observations is around 0.04 so extremely low. Thus no single observation seems to have a high impact to our model.

Based on the diagnostic plots, we can reasonably assume that our model's assumptions are valid, meaning that also the results of tests are valid.


