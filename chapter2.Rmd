# Exercises for week 2

*This week I have learned how to edit datasets, and how to represent the dataset graphically with R. I have also practiced evaluating the results from a linear regression model.*

## The dataset
First I name my dataset learning2014 and read my dataset that will be used for analysis into R.
```{r dataset}
learning2014 <- read.table("/Users/Noora/Documents/IODS-project/data/learning2014.txt", header = TRUE)
```

The dataset contains information about students who have taken an exam for a specific course (Johdatus yhteiskunta- tilastotieteeseen). In the dataset I have combined information about each participating student's gender, age, attitude (global attitude toward statistics), deep learning score, strategic learning score, surface learning score and exam points. Scores for attitude, and deep, strategic and surface learning have been formulated from each student's responses to a questionnare, and are shown in the dataset using the Likert scale with 5 levels.

I don't show the full dataset here, but it could be shown just by writing code *learning2014* in R. I show, however, the first 6 observations of the dataset just to give a reader unfamiliar with the dataset some kind of idea about the dataset. In my code I first tell R to load knitr package that I have installed. I use knitr to make the table visually better (easier to read), this is the *knitr::kable* part of the code below, and use the *head* code to show the first 6 observations.

```{r head}
library(knitr)
knitr::kable(head(learning2014))
```

I can explore the dimensions of my dataset to find out how many rows and columns it has.
```{r dimensions of dataset}
dim(learning2014)
```
This shows that the dataset has 166 rows (observations) and 7 columns (variables).

I can also explore the structure of the dataset.
```{r structure of dataset}
str(learning2014)
```
This shows again that the dataset has 166 observations and 7 variables, but the output also shows the names of the variables (e.g. gender, age) and the type of data they contain (e.g. integers for age) and some of the first observations.

## Overview and summaries of the data

Next, I look at a graphical overview of the data. I use the packages GGally and ggplot2 here.
``` {r graphical overview}
library(GGally)
library(ggplot2)
GraphicalOverview <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.5), title = "Graphical Overview", lower = list(combo = wrap("facethist", bins = 20)))
GraphicalOverview
```

I can also get summaries of all the variables in my dataset.

Summary of variable gender:
``` {r summary of gender}
summary(learning2014$gender)
```

Summary of variable age:
``` {r summary of age}
summary(learning2014$age)
```

Summary of variable attitude:
``` {r summary of attitude}
summary(learning2014$attitude)
```

Summary of variable deep:
``` {r summary of deep}
summary(learning2014$deep)
```

Summary of variable stra:
``` {r summary of stra}
summary(learning2014$stra)
```

Summary of variable surf:
``` {r summary of surf}
summary(learning2014$surf)
```

Summary of variable points:
``` {r summary of points}
summary(learning2014$points)
```
The summaries give basic information about the variables. For all variables except gender, the output shows the minimum and maximum values of the variable in my dataset, as well as other key figures, i.e. the mean, median and 1st and 3rd quartile values. For example for variable points I see that all values are between 7 and 33, and 50% of the values are between 19 and 27.75.

For gender, the output shows the frequency of events, i.e. that out of the students, 110 are females and 56 males. The first row of the graphical overview also gives this same information. The same row also gives information about the minimum, maximum, median and 1st and 3rd quartile values for each variable for the two different genders (e.g. I can see the median age of males in the dataset). These are difficult to see accurately from the graphical overview picture, however.

The graphical overview shows also correlation values for all variable pairs and the scatter plots can give some indication of the relationship between variables. For example it can be seen that when looking at attitude, the highest point values correspond to high attitude values and vice versa. For age, however, both the highest and lowest point values seem to correspond to relatively low age values. Thus I can assume that there is a relationship between attitude and points but not between age and points.

## Creating a regression model

Next I will create a regression model with three explanatory variables age, gender and attitude, and exam points as the target variable. In other words, I am trying to invstigate if the variables age, gender and attitude can be used to predict the value of exam points. Earlier I saw that attitude might be related to points but age most likely isn't, so let's see what this regression model reveals!

I will conduct some tests on my model to see if the three variables can be used to predict the value of exam points or not. Let's view the summary of the model:
``` {r multiple regression summary}
Model <- lm(points ~ age + gender + attitude, data = learning2014)
summary(Model)
```
The result shows that with this model, it is possible to explain about 20% of the variance of points. The regression line intercepts the y-axis at y-value 13.4 and attitude is positively related to points (increase in attitude increases points) whereas the other variables are negatively related.

The results of this test indicate that the null hypothesis that the variable's coefficient is 0 is valid for age and gender, as the P values of both are greater than 0.05 (0.159 and 0.720 respectively). Based on this information I should remove these two variables. Attitude, however, seems to be statistically significant and can be left in the model. Of course there might be reasons why one of the other variables should be kept in the model and not removed just because of its P-value but I don't have any expertise in this topic that could help me justify keeping variables with high P values.

## Second regression model
Let's conduct the same test but this time with just the variable attitude.
``` {r multiple regression summary2}
Model2 <- lm(points ~ attitude, data = learning2014)
summary(Model2)
```
According to this test, attitude is statistically significant, so I keep it in out model!

## The relationships between the variables in the model
Based on the output of the summary I see that the effect of attitude on exam points is 3.5 and the variables are positively related. In other words, if attitude improves by 1, exam points improve by 3.5.

I can also see the value of parameter *a* (alpha), i.e. the point where the regression line intercepts the y-axis. This value in my model is 11.6. This implies that a person with attitude 0 would get 11.6 points from the exam. I can also see that the standard error of attitude is 0.6 and the P-value very small (4.12 x 10^-09).

The multiple r-squared value tells how well the model explains the target variable exam points. I see that around 19% of the variation of points is explained by attitude. I can say that attitude seems to contribute to success in the exam (exam points), but there are also clearly other variables that affect success in the exam as well.

## Diagnostic plots

I can also draw diagnostic plots about my model. I want the diagnostic plots to sho residuals vs. fitted values, normal QQ-plot and residuals vs leverage. These will be explained in analysis of the diagnostic plots.
``` {r multiple regression diagnostics}
par(mfrow=c(2,2))
plot(Model2, which = c(1, 2, 5))
```

Because my model is about linear regression, it assumes that there is a linear relationship between attitude and exam points. Another important assumption is that there is a random variable (*e*) that adds noise to the observations This *e* describes the errors of the model. The linear regression model has several assumptions related to the errors. It is assumed that the errors 1) are normally distributed, 2) aren't correlatd, and 3) have constant variance (i.e. the size of an error doesn't depend on the explanatory variables, in this case attitude).

Analysing the residuals of the model allows me to investigate the validity of these assumptions.

The first plot, residuals vs. fitted, can be used to evaluate if the errors have constant variance. If there is any pattern in the scatter plot, there is a problem with the assumptions of the model. However, there doesn't appear to be any pattern in my scatter plot, the spread of points seems random. Thus the plot doesn't imply there would be problems with my model's assumptions.

The second plot, normal QQ, can be used to evaluate the normality of the errors. The better the points fit to the line in the plot, the better they fit the normality assumption. From the QQ-plot I can see that most of the findings fit the normality assumption, except for in the beginning and end of the line, where there is more deviation from the line. Thus the normality assumption might be questionable.

The third plot, residuals vs. leverage, can be used to see how great impact a single obsrvation has on the model. There might be e.g. one observation that is causing the slope of the regression line to significantly change. In my plot, there are a couple of observations at the far right side of the plot. However, the leverage of these observations is around 0.04 so extremely low. Thus no single observation seems to have a high impact to my model.

Based on the diagnostic plots, I can reasonably assume that my model's assumptions are valid, meaning that also the results of tests are valid.


